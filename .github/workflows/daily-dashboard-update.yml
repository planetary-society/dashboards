# GitHub Action Workflow: Daily Dashboard Update
#
# This workflow runs daily to:
# 1. Download the latest CSV data for the cancellations dashboard.
# 2. Compare and clean up data files.
# 3. Commit new data if changed.
# 4. Deploy the static D3.js dashboards to GitHub Pages.

name: Daily Dashboard Update

on:
  schedule:
    # runs every day at 10:00 AM Pacific Time (15:00 UTC)
    - cron: '0 17 * * *'
  workflow_dispatch: # Allows manual triggering

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: write   # Required to commit changes
      pages: write      # Required to deploy to GitHub Pages
      id-token: write   # Required for trusted deployment

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Ensure data directory exists
        run: mkdir -p data

      - name: Define new filename
        id: define_filename
        run: echo "FILENAME=data/nasa_cancelled_contracts_$(date +'%Y-%m-%d').csv" >> $GITHUB_OUTPUT

      - name: Download latest CSV from GitHub
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          # Find the latest CSV using gh api, download via raw URL (avoids 1MB API limit)
          LATEST_FILE=$(gh api repos/planetary-society/nasa-cancellations-tracking/contents/consolidated \
            --jq '[.[] | select(.name | endswith(".csv"))] | sort_by(.name) | last | .name')

          echo "Latest source file: $LATEST_FILE"
          curl -fL -o "${{ steps.define_filename.outputs.FILENAME }}" \
            "https://raw.githubusercontent.com/planetary-society/nasa-cancellations-tracking/main/consolidated/${LATEST_FILE}"

      - name: Compare with previous and cleanup
        id: compare_files
        run: |
          NEW_FILE="${{ steps.define_filename.outputs.FILENAME }}"
          echo "Newly downloaded file: $NEW_FILE"
          PREVIOUS_FILE=$(ls -r data/*.csv 2>/dev/null | grep -v "$NEW_FILE" | head -n 1)
          if [[ -n "$PREVIOUS_FILE" && -f "$PREVIOUS_FILE" ]]; then
            echo "Comparing with previous file: $PREVIOUS_FILE"
            if cmp -s "$PREVIOUS_FILE" "$NEW_FILE"; then
              echo "Files are identical. Deleting $NEW_FILE."
              rm "$NEW_FILE"
              echo "file_kept=false" >> $GITHUB_OUTPUT
            else
              echo "Files are different. Keeping $NEW_FILE."
              echo "file_kept=true" >> $GITHUB_OUTPUT
            fi
          else
            echo "No previous file found. Keeping $NEW_FILE."
            echo "file_kept=true" >> $GITHUB_OUTPUT
          fi

      - name: Copy latest data to docs folder
        run: |
          LATEST_FILE=$(ls -t data/nasa_cancelled_contracts_*.csv 2>/dev/null | head -n 1)
          if [[ -n "$LATEST_FILE" ]]; then
            mkdir -p docs/data/cancellations
            cp "$LATEST_FILE" docs/data/cancellations/nasa_cancelled_contracts_latest.csv
            echo "Copied $LATEST_FILE to docs/data/cancellations/nasa_cancelled_contracts_latest.csv"

            # Extract date from filename and write metadata (overwrites each time)
            DATE=$(echo "$LATEST_FILE" | grep -oE '[0-9]{4}-[0-9]{2}-[0-9]{2}')
            echo "{\"lastUpdated\": \"$DATE\"}" > docs/data/cancellations/metadata.json
            echo "Wrote last updated date: $DATE to metadata.json"
          fi

      - name: Commit & push changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore: daily data refresh"
          file_pattern: |
            data/*.csv
            docs/data/**/*.csv

      - name: Configure GitHub Pages
        uses: actions/configure-pages@v5

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'docs'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
