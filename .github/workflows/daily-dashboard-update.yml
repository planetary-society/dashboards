# GitHub Action Workflow: Daily Dashboard Update
#
# This workflow runs daily to:
# 1. Download the latest CSV data for the cancellations dashboard.
# 2. Compare and clean up data files.
# 3. Commit new data if changed.
# 4. Deploy the static D3.js dashboards to GitHub Pages.

name: Daily Dashboard Update

on:
  schedule:
    # runs every day at 10:00 AM Pacific Time (15:00 UTC)
    - cron: '0 17 * * *'
  workflow_dispatch: # Allows manual triggering

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: write   # Required to commit changes
      pages: write      # Required to deploy to GitHub Pages
      id-token: write   # Required for trusted deployment

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Download latest CSV from GitHub
        id: download
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          mkdir -p data

          # Find the latest CSV filename and extract date
          LATEST_FILE=$(gh api repos/planetary-society/nasa-cancellations-tracking/contents/consolidated \
            --jq '[.[] | select(.name | endswith(".csv"))] | sort_by(.name) | last | .name')
          DATE=$(echo "$LATEST_FILE" | grep -oE '[0-9]{4}-[0-9]{2}-[0-9]{2}')

          echo "Latest source file: $LATEST_FILE (date: $DATE)"
          echo "source_date=$DATE" >> $GITHUB_OUTPUT

          # Download directly to data directory
          curl -fL -o "data/nasa_cancelled_contracts_${DATE}.csv" \
            "https://raw.githubusercontent.com/planetary-society/nasa-cancellations-tracking/main/consolidated/${LATEST_FILE}"

      - name: Compare and update if changed
        id: compare
        run: |
          DOCS_FILE="docs/data/cancellations/nasa_cancelled_contracts_latest.csv"
          DATA_FILE="data/nasa_cancelled_contracts_${{ steps.download.outputs.source_date }}.csv"
          mkdir -p docs/data/cancellations

          echo "DATA_FILE: $DATA_FILE"
          echo "DOCS_FILE: $DOCS_FILE"
          echo "Data file exists: $(test -f "$DATA_FILE" && echo yes || echo no)"
          echo "Docs file exists: $(test -f "$DOCS_FILE" && echo yes || echo no)"

          if [[ -f "$DOCS_FILE" ]] && cmp -s "$DATA_FILE" "$DOCS_FILE"; then
            echo "No changes detected - files are identical"
            echo "changed=false" >> $GITHUB_OUTPUT
            rm "$DATA_FILE"
          else
            echo "Changes detected, updating files"
            echo "changed=true" >> $GITHUB_OUTPUT

            # Remove old cancellation CSVs from data dir, keep only the new one
            find data -name "nasa_cancelled_contracts_*.csv" ! -name "$(basename $DATA_FILE)" -delete

            # Update docs with new file
            cp -v "$DATA_FILE" "$DOCS_FILE"

            # Update metadata
            echo "{\"lastUpdated\": \"${{ steps.download.outputs.source_date }}\"}" > docs/data/cancellations/metadata.json
            echo "Updated metadata: ${{ steps.download.outputs.source_date }}"

            # Verify files were updated
            echo "Docs file after copy:"
            head -1 "$DOCS_FILE"
            ls -la docs/data/cancellations/
          fi

      - name: Commit & push changes
        if: steps.compare.outputs.changed == 'true'
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore: daily data refresh"
          file_pattern: "data/*.csv docs/data/cancellations/*"

      - name: Configure GitHub Pages
        uses: actions/configure-pages@v5

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'docs'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
